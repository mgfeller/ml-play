{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From:\n",
    "https://gist.github.com/katsugeneration/b492a548aa342ce0badfe1eed86ea5a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_DATA_SIZE = 4\n",
    "CLASS_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv data for iris data format\n",
    "def load_csv(filename):\n",
    "    file = pd.read_csv(filename, header=0)\n",
    "\n",
    "    # get sample's metadata\n",
    "    n_samples = int(file.columns[0])\n",
    "    n_features = int(file.columns[1])\n",
    "\n",
    "    # divide samples into explanation variables and target variable\n",
    "    data = np.empty((n_samples, n_features))\n",
    "    target = np.empty((n_samples,), dtype=np.int)\n",
    "    for i, row in enumerate(file.itertuples()):\n",
    "        target[i] = np.asarray(row[-1], dtype=np.int)\n",
    "        data[i] = np.asarray(row[1:n_features+1], dtype=np.float64)\n",
    "    return (data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output train data \n",
    "def get_batch_data(x_train, y_train, size=None):\n",
    "    if size is None:\n",
    "        size = len(x_train)\n",
    "    batch_xs = x_train\n",
    "    batch_ys = []\n",
    "\n",
    "    # convert to 1-of-N vector\n",
    "    for i in range(len(y_train)):\n",
    "        val = np.zeros((3), dtype=np.float64)\n",
    "        val[y_train[i]] = 1.0\n",
    "        batch_ys.append(val)\n",
    "    batch_ys = np.asarray(batch_ys)\n",
    "    return batch_xs[:size], batch_ys[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output test data\n",
    "def get_test_data(x_test, y_test):\n",
    "    batch_ys = []\n",
    "\n",
    "    # convert to 1-of-N vector\n",
    "    for i in range(len(y_test)):\n",
    "        val = np.zeros((3), dtype=np.float64)\n",
    "        val[y_test[i]] = 1.0\n",
    "        batch_ys.append(val)\n",
    "    return x_test, np.asarray(batch_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parameter initialize\n",
    "def get_stddev(in_dim, out_dim):\n",
    "    return 1.3 / math.sqrt(float(in_dim) + float(out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Model Class\n",
    "class Classifier:\n",
    "    def __init__(self, hidden_units=[10], n_classes=0):\n",
    "        self._hidden_units = hidden_units\n",
    "        self._n_classes = n_classes\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "    # build model\n",
    "    def inference(self, x):\n",
    "        hidden = []\n",
    "\n",
    "        # Input Layer\n",
    "        with tf.name_scope(\"input\"):\n",
    "            weights = tf.Variable(tf.truncated_normal([IRIS_DATA_SIZE, self._hidden_units[0]], stddev=get_stddev(IRIS_DATA_SIZE, self._hidden_units[0])), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([self._hidden_units[0]]), name='biases')\n",
    "            input = tf.matmul(x, weights) + biases\n",
    "\n",
    "        # Hidden Layers\n",
    "        for index, num_hidden in enumerate(self._hidden_units):\n",
    "            if index == len(self._hidden_units) - 1: break\n",
    "            with tf.name_scope(\"hidden{}\".format(index+1)):\n",
    "                weights = tf.Variable(tf.truncated_normal([num_hidden, self._hidden_units[index+1]], stddev=get_stddev(num_hidden, self._hidden_units[index+1])), name='weights')\n",
    "                biases = tf.Variable(tf.zeros([self._hidden_units[index+1]]), name='biases')\n",
    "                inputs = input if index == 0 else hidden[index-1]\n",
    "                hidden.append(tf.nn.relu(tf.matmul(inputs, weights) + biases, name=\"hidden{}\".format(index+1)))\n",
    "        \n",
    "        # Output Layer\n",
    "        with tf.name_scope('output'):\n",
    "            weights = tf.Variable(tf.truncated_normal([self._hidden_units[-1], self._n_classes], stddev=get_stddev(self._hidden_units[-1], self._n_classes)), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([self._n_classes]), name='biases')\n",
    "            logits = tf.nn.softmax(tf.matmul(hidden[-1], weights) + biases)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    # loss function\n",
    "    def loss(self, logits, y):        \n",
    "        return -tf.reduce_mean(y * tf.log(logits))\n",
    "\n",
    "    # fitting function for train data\n",
    "    def fit(self, x_train=None, y_train=None, steps=200):\n",
    "        # build model\n",
    "        x = tf.placeholder(tf.float32, [None, IRIS_DATA_SIZE])\n",
    "        y = tf.placeholder(tf.float32, [None, CLASS_SIZE])\n",
    "        logits = self.inference(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        train_op = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "        # save variables\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._logits = logits\n",
    " \n",
    "        # init parameters\n",
    "        init = tf.initialize_all_variables() \n",
    "        self._sess.run(init)\n",
    "\n",
    "        # train\n",
    "        for i in range(steps):\n",
    "            batch_xs, batch_ys = get_batch_data(x_train, y_train)\n",
    "            self._sess.run(train_op, feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "    # evaluation function for test data\n",
    "    def evaluate(self, x_test=None, y_test=None):\n",
    "        x_test, y_test = get_test_data(x_test, y_test)\n",
    "        \n",
    "        # build accuracy calculate step\n",
    "        correct_prediction = tf.equal(tf.argmax(self._logits, 1), tf.argmax(self._y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # evaluate\n",
    "        return self._sess.run([accuracy], feed_dict={self._x: x_test, self._y: y_test})\n",
    "\n",
    "    # label pridiction\n",
    "    def predict(self, samples):\n",
    "        predictions = tf.argmax(self._logits, 1)\n",
    "        return self._sess.run(predictions, {self._x: samples})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "x_train, y_train = load_csv(filename=IRIS_TRAINING)\n",
    "x_test, y_test = load_csv(filename=IRIS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = Classifier(hidden_units=[10, 20, 10], n_classes=CLASS_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\bin\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Fit model.\n",
    "classifier.fit(x_train, y_train, steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.266667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(x_test, y_test)[0]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2]\n"
     ]
    }
   ],
   "source": [
    "# Classify two new flower samples.\n",
    "new_samples = np.array([[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\n",
    "y = classifier.predict(new_samples)\n",
    "print ('Predictions: {}'.format(str(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
